{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h67IyVsP9NMq"
      },
      "source": [
        "# ML NN Project: Convolutional Neural Network\n",
        "\n",
        "In this project you will complete the provided Python code, using PyTorch, to perform image classification on the Fashion-MNIST and MNIST Digits datasets using various CNN architectures. In the following we provide the code skeleton for working with the Fashion-MNIST dataset.  After completing that part of the project, in Part 5 below you will repeat experimentations with the MNIST Digits dataset.\n",
        "\n",
        "## **Important!**\n",
        "# Make sure you change your runtime type to GPU! This can be found in the top menu following \"Runtime->Change runtime type\" then select from the dropdown T4 GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part A - Code skeleton for experimenting with MNIST Fashion dataset**"
      ],
      "metadata": {
        "id": "8cC0BsjeJ94m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET0x3Edic4sg"
      },
      "source": [
        "## Imports, Data, and Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8mFRY-Y_Ok9"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3GN4cEe_ZhO"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maFMqmYg_wKt"
      },
      "source": [
        "### Data (for Fashion-MNIST)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JcERc1O_xvO"
      },
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "print()\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# get info about data\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7Zli2y9CrHK"
      },
      "source": [
        "### Global Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXLDy0huCunw"
      },
      "source": [
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI_CXCYm9jbl"
      },
      "source": [
        "## **Part A.1 - The Effect of Filters**\n",
        "\n",
        "In this section you will be comparing the relative performance and training times for two different CNN architectures.\n",
        "\n",
        "Both architectures will have the following layers in order:\n",
        "\n",
        "``` Python\n",
        "Conv2D <- input layer\n",
        "ReLU\n",
        "Conv2D\n",
        "ReLU\n",
        "MaxPooling2D((2,2))\n",
        "Conv2D\n",
        "ReLU\n",
        "Conv2D\n",
        "ReLU\n",
        "MaxPooling2D((2,2))\n",
        "Flatten\n",
        "Linear(1024 nodes)\n",
        "ReLU\n",
        "Linear(num_classes) <- output layer\n",
        "```\n",
        "\n",
        "In ```build_model_1A()``` the first two Conv2D layers should have 16 filters, and the second two Conv2D layers should have 32 filters.\n",
        "\n",
        "In ```build_model_1B()``` the first two Conv2d layers should have 64 filters, and the second two Conv2D layers should have 128 filters.\n",
        "\n",
        "All the Conv2D layers should have ```padding=\"same\"``` and ```kernel_size=(3,3)``` passed as parameters.\n",
        "\n",
        "Finally both max pooling layers should have a pool size of (2, 2)\n",
        "\n",
        "**Documentation for the each layer type:**\n",
        "\n",
        "*Conv2D*: https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n",
        "\n",
        "*MaxPooling2D*: https://docs.pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
        "\n",
        "*Flatten*: https://docs.pytorch.org/docs/stable/generated/torch.nn.modules.flatten.Flatten.html\n",
        "\n",
        "*Linear*: https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
        "\n",
        "*ReLU*: https://docs.pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
        "\n",
        "**Documentation for Sequential model**: https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zvsQ3bxBkpv"
      },
      "source": [
        "### ***TODO*** Model 1A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEPzweNg9FZ3"
      },
      "source": [
        "def build_model_1A():\n",
        "  # !!! Define Model 1A !!!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C605QLGFCBbL"
      },
      "source": [
        "### ***TODO*** Model 1B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujotmsx7CDKc"
      },
      "source": [
        "def build_model_1B():\n",
        "  # !!! Define Model 1B !!!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX6a0n4YCaLz"
      },
      "source": [
        "### ***TODO*** Model Compilation and Summaries\n",
        "\n",
        "Compile and print summaries for the two architectures.\n",
        "\n",
        "When compiling the models, use Adam as the optimizer with ```LEARNING_RATE```, sparse categorical crossentropy as the loss function (may have to manually set ```from_logits```), and accuracy as the only performance metric."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !!! Get accelerator device !!!\n",
        "device = torch.accelerator.current_accelerator().type if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# !!! Send networks to gpu !!!\n",
        "model_1a = build_model_1A().to(device)\n",
        "model_1b = build_model_1B().to(device)\n",
        "\n",
        "# !!! Print summaries !!!\n",
        "print(model_1a)\n",
        "print(model_1b)"
      ],
      "metadata": {
        "id": "1g0g6CFjzka5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUpK13JZEKW1"
      },
      "source": [
        "### Testing Helper Method\n",
        "\n",
        "Method to help with testing. **Do not change.**\n",
        "\n",
        "When compiling the model, this method uses Adam as the optimizer with ```LEARNING_RATE``` and sparse categorical crossentropy as the loss function.\n",
        "\n",
        "*CrossEntropyLoss*: https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvGTmm0gEQ5k"
      },
      "source": [
        "def perf_and_test(model, model_name):\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  batches = [0]\n",
        "  start = time.time()\n",
        "\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  for epoch in range(EPOCHS):\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # Compute prediction error\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "\n",
        "      # Backpropagation\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "      accuracy = correct / len(X)\n",
        "      losses.append(loss)\n",
        "      accuracies.append(accuracy)\n",
        "      if (epoch != 0) or (batch != 0):\n",
        "        batches.append(batches[-1] + 1)\n",
        "      if batch % 500 == 0:\n",
        "        print(f\"batch: {batch}, loss: {loss:<7.4f}, accuracy: {accuracy:<5.2f}\")\n",
        "  print()\n",
        "  print(\"\" + model_name + \" testing information\")\n",
        "  print(\"Training took \", round((time.time() - start),2), \"seconds\")\n",
        "\n",
        "  print(\"Testing statistics\")\n",
        "  print(accuracies[0:10])\n",
        "  size = len(test_dataloader.dataset)\n",
        "  num_batches = len(test_dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in test_dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error:\")\n",
        "  print(f\"Avg loss: {test_loss:>5.2f}, Accuracy: {(100*correct):>7.4f}%\")\n",
        "\n",
        "  fig,ax = plt.subplots()\n",
        "  ax.plot(batches, accuracies, color=\"red\")\n",
        "  ax.set_xlabel(\"batches\",fontsize=14)\n",
        "  ax.set_ylabel(\"accuracy\", color=\"red\", fontsize=14)\n",
        "  ax2=ax.twinx()\n",
        "  ax2.plot(batches, losses, color=\"blue\")\n",
        "  ax2.set_ylabel(\"loss\", color=\"blue\", fontsize=14)\n",
        "  plt.title(\"Training Acc. and Loss: \" + model_name)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjt9DwSMXgMo"
      },
      "source": [
        "### ***TODO*** Global Hyperparameters\n",
        "\n",
        "Start with a number of epochs that seems too high Ex:(between 30 and 80). After training, examine the loss plot to identify a point of diminishing returns in training, and then set the number of training epochs equal to that. Note any changes in testing accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFI31S_WXgMp"
      },
      "source": [
        "EPOCHS = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni9W075_EkLR"
      },
      "source": [
        "### Testing models\n",
        "\n",
        "The following code cell will run both models and provided statistics for the training time, testing accuracy, and testing loss of the models. The loss function used is cross-entropy loss. It also generates graphs for training accuracy and loss over time. These graphs can be used in your report.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpurcAUFEsoE"
      },
      "source": [
        "perf_and_test(model_1a, \"Model 1A\")\n",
        "perf_and_test(model_1b, \"Model 1B\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saAP6dZ-H1At"
      },
      "source": [
        "## **Part A.2 - Dropout Layers**\n",
        "\n",
        "In part 2, we will be examining the effect of dropout layers on the performance of the model. Copy your code from the ```build_model_1B()``` method into the ```build_model_2()``` method, and then add dropout layers after each max pooling layer. Model 2 architecture should be:\n",
        "\n",
        "```\n",
        "Conv2D\n",
        "ReLU\n",
        "Conv2D\n",
        "ReLU\n",
        "MaxPooling2D((2,2))\n",
        "Dropout(0.5) <- Add this\n",
        "Conv2D\n",
        "ReLU\n",
        "Conv2D\n",
        "ReLU\n",
        "MaxPooling2D((2,2))\n",
        "Dropout(0.5) <- Add this\n",
        "Flatten\n",
        "Linear(1024 nodes)\n",
        "ReLU\n",
        "Linear(num_classes, softmax)\n",
        "```\n",
        "\n",
        "Note that the we are passing ```0.5``` as an argument to the dropout layers. This means that 50% of the connection between the max pooling and next layer will be dropped.\n",
        "\n",
        "*Documentation on dropout layers can be found here*: https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnS9xxNUJrln"
      },
      "source": [
        "### ***TODO*** Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IkD65YIJzSw"
      },
      "source": [
        "def build_model_2():\n",
        "  # Implement your model here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICtYPK620W_h"
      },
      "source": [
        "### ***TODO*** Model Creation and Summary\n",
        "\n",
        "Create the model, send it to the gpu, and print a summary for the architecture."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zRA2rDV3vJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s5mSscjYLJH"
      },
      "source": [
        "### **TODO** Global Hyperparameters\n",
        "\n",
        "Dropout layers, while capable of making the model more robust, also require more training. You should again increase the number of epochs to find a point of diminishing returns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5J5d92TYLJJ"
      },
      "source": [
        "EPOCHS = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk8Qz2T1Ku1M"
      },
      "source": [
        "### Testing Model 2\n",
        "\n",
        "When looking at your results, pay attention to the difference between the training and test accuracy of Model 1B to the difference of the two accuracies in Model 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCKGZ652K40G"
      },
      "source": [
        "perf_and_test(model_2, \"Model 2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djGQRfobLJr3"
      },
      "source": [
        "## **Part A.3 - Batch Normalization**\n",
        "\n",
        "In part 3, we will be examining the effect of batch normalization layers on the performance of the model. Copy your code from the ```build_model_1B()``` method into the ```build_model_3()``` method, and then add batch normalization layers after each Conv2D layer. The Model 3 architecture should be:\n",
        "\n",
        "```\n",
        "Conv2D\n",
        "ReLU\n",
        "BatchNormalization\n",
        "ReLU\n",
        "Conv2D\n",
        "BatchNormalization\n",
        "MaxPooling2D((2,2))\n",
        "Conv2D\n",
        "ReLU\n",
        "BatchNormalization\n",
        "Conv2D\n",
        "ReLU\n",
        "BatchNormalization\n",
        "MaxPooling2D((2,2))\n",
        "Flatten\n",
        "Dense(1024 nodes)\n",
        "ReLU\n",
        "Dense(num_classes, softmax)\n",
        "```\n",
        " BatchNormalization layers requires no parameters.\n",
        "\n",
        " *Documentation can be found here*: https://keras.io/api/layers/normalization_layers/batch_normalization/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bnDQSWzNLB2"
      },
      "source": [
        "### ***TODO*** Model 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0Ij7PNUNLB3"
      },
      "source": [
        "def build_model_3():\n",
        "  # Copy your code from build_model_1B here and add BatchNormalization layers as specified"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2lT-iky1Wgw"
      },
      "source": [
        "### ***TODO*** Model Creation and Summary\n",
        "\n",
        "Create the model, send it to the gpu, and print a summary for the architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZCSh8JE1WhE"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W40rVSoeZAlt"
      },
      "source": [
        "### ***TODO*** Global Hyperparameters\n",
        "\n",
        " Batch normalization has been shown to improve convergence in neural networks, and so the number of training epochs required may be lower than in the other architectures. Again refer to the initial loss plot to find the point of diminishing returns and update the ```EPOCHS``` variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUivow6MZAlv"
      },
      "source": [
        "EPOCHS = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8ZBMiSlNoSu"
      },
      "source": [
        "### Testing Model 3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR_FDdEMNoSv"
      },
      "source": [
        "perf_and_test(model_3, \"Model 3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pI2YZw9OCmI"
      },
      "source": [
        "## **Part A.4 - Layer count**\n",
        "\n",
        "In the final part of this project, you will be looking out how the number of layers affect performance and training time.\n",
        "\n",
        "In ```build_model_4```, you need to implement the following architecture:\n",
        "\n",
        "```python\n",
        "# block 1 - 64 filters per Conv2D\n",
        "conv2D\n",
        "ReLU\n",
        "BatchNormaliztion\n",
        "conv2D\n",
        "ReLU\n",
        "BatchNormalization\n",
        "conv2D\n",
        "ReLU\n",
        "BatchNormalization\n",
        "MaxPooling2D\n",
        "Dropout(0.5)\n",
        "#block 2 - 128 filters per Conv2D\n",
        "conv2D\n",
        "ReLU\n",
        "BatchNormaliztion\n",
        "conv2D\n",
        "ReLU\n",
        "BatchNormalization\n",
        "conv2D\n",
        "ReLU\n",
        "BatchNormalization\n",
        "MaxPooling2D\n",
        "Dropout(0.5)\n",
        "#block 3 - 256 filters per Conv2D\n",
        "conv2D\n",
        "ReLU\n",
        "BatchNormaliztion\n",
        "conv2D\n",
        "ReLU\n",
        "BatchNormalization\n",
        "conv2D\n",
        "ReLU\n",
        "BatchNormalization\n",
        "MaxPooling2D\n",
        "Dropout(0.5)\n",
        "# Dense layers\n",
        "Flatten\n",
        "Dense(1024)\n",
        "ReLU\n",
        "Dense(512)\n",
        "ReLU\n",
        "Dense(num_classes, softmax)\n",
        "```\n",
        "\n",
        "Notice each block consists of 3 conv2d layers, with 3 batch normalization layers, a maxpooling and dropout layer. The first block should have 64 filters for each Conv2d layer, the second should have 128 filters, and the third should have 256. Also, we are adding an additional dense layer with 512 nodes to the model architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJh472usZ9S-"
      },
      "source": [
        "### ***TODO*** Model 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmzGoD9waJ8r"
      },
      "source": [
        "def build_model_4():\n",
        "  # Implement your model here. You can copy code from previous models as a reference."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B51tFQkVZ44z"
      },
      "source": [
        "### ***TODO*** Global Hyperparameters\n",
        "\n",
        "Larger models generally require more training as there are more parameters, so you may need to increase the number of training epochs for this model. In addition, more complicated architectures and problem domains can benefit from lower learning rates. Experiment with the learning rate to see if you notice any improvements. Keep in mind that decreasing the learning rate increases the training time, and to fully train your model you may need to increase the number of epochs as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E9CXtcDZ440"
      },
      "source": [
        "EPOCHS = 25\n",
        "LEARNING_RATE = 0.0001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEZ7vP9v2ZQY"
      },
      "source": [
        "### ***TODO*** Model Creation and Summary\n",
        "\n",
        "Create the model, send it to the gpu, and print a summary for the architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iJz4x0y2ZQh"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMjADtiLcduU"
      },
      "source": [
        "### Testing Model 4\n",
        "\n",
        "This model will likely take considerably longer to train (10-30 minutes depending on number of epochs, and whichever GPU Colab assigned to your runtime) . It may be a good time to take a coffee break, or work on your report for a bit.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YyVprVxcduV"
      },
      "source": [
        "perf_and_test(model_4, \"Model 4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUpG00t24KK_"
      },
      "source": [
        "## **Part B - Repeat for MNIST Digits dataset**\n",
        "\n",
        "Repeat Part A, this time using the MNIST Digits dataset. **Copy the code blocks above and paste below so you do not lose your previous work!**"
      ]
    }
  ]
}