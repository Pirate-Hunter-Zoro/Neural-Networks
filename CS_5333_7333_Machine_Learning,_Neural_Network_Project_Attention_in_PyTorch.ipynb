{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, you will implement scaled dot product attention, a key component to the transformer neural network architecture that has revolutionized natural language processing."
      ],
      "metadata": {
        "id": "k_7lL0LhaClo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMgWOv2yY2VD"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y transformers huggingface_hub\n",
        "!pip install -q --upgrade transformers huggingface_hub seaborn\n",
        "# DON'T CHANGE ANYTHING IN THIS CELL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "# load BERT base\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert = AutoModel.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    attn_implementation=\"eager\",\n",
        "    output_attentions=True\n",
        ")\n",
        "# DON'T CHANGE ANYTHING IN THIS CELL"
      ],
      "metadata": {
        "id": "euxWeaqbZCG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Implements scaled-dot-product attention.\n",
        "    Shapes:\n",
        "        Q, K, V : (B, H, L, d_k)   # B=batch, H=heads, L=sequence\n",
        "        mask    : broadcastable to (B, H, L, L)   (0 = pad, 1 = keep)\n",
        "    Returns:\n",
        "        out  : (B, H, L, d_k)\n",
        "        attn : (B, H, L, L)\n",
        "    \"\"\"\n",
        "\n",
        "    # ### TODO 1. raw scores = Q * Kᵀ (hint: transpose(-2, -1))\n",
        "    scores =\n",
        "\n",
        "    # ### TODO 2. scale by √d_k\n",
        "    d_k = Q.size(-1)\n",
        "    scores =\n",
        "\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    # ### TODO 3. softmax over keys dimension (-1)\n",
        "    attn =\n",
        "\n",
        "    # ### TODO 4. multiply by V\n",
        "    out =\n",
        "\n",
        "    return out, attn\n"
      ],
      "metadata": {
        "id": "1o_fGyNPZK2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model=768, num_heads=8):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must split evenly\"\n",
        "        self.h   = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, L, C = x.shape\n",
        "\n",
        "        # ### TODO 1-3. project & reshape for heads\n",
        "        Q = self.W_q(x).view(B, L, self.h, self.d_k).transpose(1, 2)\n",
        "        K =\n",
        "        V =\n",
        "\n",
        "        # ### TODO 4. call single-head attention\n",
        "        out, attn =\n",
        "\n",
        "        # ### TODO 5-6. merge heads & final linear\n",
        "        out = out.transpose(1, 2).contiguous().view(B, L, C)\n",
        "        out =\n",
        "\n",
        "        return out, attn\n"
      ],
      "metadata": {
        "id": "PrLEKGloawIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def embed_sentence(text):\n",
        "    ids   = tokenizer(text, return_tensors=\"pt\")\n",
        "    emb   = bert.embeddings(ids[\"input_ids\"])\n",
        "    mask  = ids[\"attention_mask\"].unsqueeze(1).unsqueeze(2)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(ids[\"input_ids\"][0])\n",
        "    return emb, mask, tokens\n",
        "\n",
        "def plot_heads(attn, tokens, heads_per_row=4):\n",
        "    \"\"\"\n",
        "    attn: (B=1, H, L, L)   attention from our MHA\n",
        "    \"\"\"\n",
        "    attn = attn[0].cpu()\n",
        "    H, L, _ = attn.shape\n",
        "    rows = (H + heads_per_row - 1)//heads_per_row\n",
        "    fig, axes = plt.subplots(rows, heads_per_row, figsize=(4*heads_per_row,4*rows))\n",
        "    axes = axes.flatten()\n",
        "    for h in range(H):\n",
        "        sns.heatmap(attn[h], vmin=0, vmax=attn.max(),\n",
        "                    cmap=\"viridis\", square=True,\n",
        "                    xticklabels=tokens, yticklabels=tokens,\n",
        "                    ax=axes[h], cbar=False)\n",
        "        axes[h].set_title(f\"Head {h}\")\n",
        "        axes[h].tick_params(labelsize=8, rotation=90)\n",
        "    for ax in axes[H:]:\n",
        "        ax.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    # DON'T CHANGE ANYTHING IN THIS CELL"
      ],
      "metadata": {
        "id": "raP1axP9a3A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check our implementation (it isn't trained so the attention will be all over the place.)\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\" # ### TODO: Use a unique sentence!\n",
        "x, mask, toks = embed_sentence(sentence)\n",
        "\n",
        "mha = MultiHeadAttention(d_model=768, num_heads=12)\n",
        "mha.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    out, attn = mha(x, mask)\n",
        "\n",
        "plot_heads(attn, toks)"
      ],
      "metadata": {
        "id": "kdR7DL41a6hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See attention directly using BERT.\n",
        "\n",
        "x, mask, toks = embed_sentence(sentence)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = bert(**tokenizer(sentence, return_tensors=\"pt\"), output_attentions=True)\n",
        "    attn = outputs.attentions[0]\n",
        "plot_heads(attn, toks, heads_per_row=4)"
      ],
      "metadata": {
        "id": "hJQG7vs0bd9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use pretrained BERT weights to test our implementation.\n",
        "\n",
        "bert_sa   = bert.encoder.layer[0].attention.self\n",
        "bert_dout = bert.encoder.layer[0].attention.output.dense\n",
        "\n",
        "mha_pt = MultiHeadAttention(d_model=768, num_heads=12)\n",
        "\n",
        "with torch.no_grad():\n",
        "    mha_pt.W_q.weight.copy_(bert_sa.query.weight)\n",
        "    mha_pt.W_q.bias.copy_( bert_sa.query.bias )\n",
        "    mha_pt.W_k.weight.copy_(bert_sa.key.weight)\n",
        "    mha_pt.W_k.bias.copy_( bert_sa.key.bias )\n",
        "    mha_pt.W_v.weight.copy_(bert_sa.value.weight)\n",
        "    mha_pt.W_v.bias.copy_( bert_sa.value.bias )\n",
        "    mha_pt.W_o.weight.copy_(bert_dout.weight)\n",
        "    mha_pt.W_o.bias.copy_( bert_dout.bias )\n",
        "\n",
        "mha_pt.eval()\n",
        "with torch.no_grad():\n",
        "    out_pt, attn_pt = mha_pt(x, mask)\n",
        "plot_heads(attn_pt, toks)"
      ],
      "metadata": {
        "id": "C5VCcx2acPNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize a different layer with layer_id.\n",
        "\n",
        "layer_id   = 11 # TODO: Visualize different layers!\n",
        "layer      = bert.encoder.layer[layer_id]\n",
        "\n",
        "bert_sa    = layer.attention.self\n",
        "bert_dout  = layer.attention.output.dense\n",
        "mha_L      = MultiHeadAttention(d_model=768, num_heads=12)\n",
        "\n",
        "with torch.no_grad():\n",
        "    mha_L.W_q.weight.copy_(bert_sa.query.weight);  mha_L.W_q.bias.copy_(bert_sa.query.bias)\n",
        "    mha_L.W_k.weight.copy_(bert_sa.key.weight);    mha_L.W_k.bias.copy_(bert_sa.key.bias)\n",
        "    mha_L.W_v.weight.copy_(bert_sa.value.weight);  mha_L.W_v.bias.copy_(bert_sa.value.bias)\n",
        "    mha_L.W_o.weight.copy_(bert_dout.weight);      mha_L.W_o.bias.copy_(bert_dout.bias)\n",
        "\n",
        "mha_L.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    x, mask, toks = embed_sentence(sentence)\n",
        "    _, attn_L     = mha_L(x, mask)\n",
        "\n",
        "plot_heads(attn_L, toks, heads_per_row=4)"
      ],
      "metadata": {
        "id": "-FZdjQWVekC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are interested, here is an assignment to help you use pretrained transformer networks on next text generation, sentiment analysis, and masked token prediction. This is not required: https://colab.research.google.com/drive/1WvWL9ZxYd1PgJvMFdfT0Ye389TlQFrzA?usp=sharing"
      ],
      "metadata": {
        "id": "EXdl9gbxaxLy"
      }
    }
  ]
}